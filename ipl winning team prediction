import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt 

# Load all CSV files from the specified folder
folder_path = 'C:\\Users\\ASUS\\Downloads\\iplDatasets'
files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]

# Assuming there is only one CSV file, you can load it like this
file_path = os.path.join(folder_path, files[0])
data = pd.read_csv(file_path)

# Assuming 'total_runs' is your target variable
target_variable = 'total_runs'

# Drop unnecessary columns and handle categorical variables
data = data.drop(['match_id', 'player_dismissed', 'dismissal_kind', 'fielder'], axis=1)

# Label encode categorical variables
label_encoder = LabelEncoder()
for col in data.columns:
    if data[col].dtype == 'object':
        data[col] = label_encoder.fit_transform(data[col])

# Drop the target variable from the features
X = data.drop(target_variable, axis=1)

# The target variable (what you want to predict)
y = data[target_variable]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Fit the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
predictions = model.predict(X_test)

# Evaluate the model performance
mae = mean_absolute_error(y_test, predictions)
print(f'Mean Absolute Error: {mae}')

# Feature importances
feature_importances = model.feature_importances_
indices = feature_importances.argsort()

plt.barh(range(len(indices)), feature_importances[indices], align='center')
plt.yticks(range(len(indices)), [X.columns[i] for i in indices])  # Assuming X is a DataFrame
plt.xlabel('Feature Importance')
plt.title('Random Forest Feature Importances')
plt.show()
